{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import scipy.io as sio\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for individual unit analysis\n",
    "def tuning_calculate(firingdata,angle_list):\n",
    "    divisions = 50\n",
    "    dtheta = 1/divisions*360\n",
    "    theta = np.arange(0,divisions)/divisions*360\n",
    "    tuning_fr = np.zeros_like(theta)\n",
    "    for i in np.arange(0,divisions):\n",
    "        idx = (angle_list>i*dtheta)&(angle_list<(i+1)*dtheta)\n",
    "        tuning_fr[i] = np.mean(firingdata[idx]) \n",
    "    return tuning_fr\n",
    "def fwhm_calculate(tuning_curve):\n",
    "    data = tuning_curve-np.min(tuning_curve)\n",
    "    max_indx = np.argmax(data)\n",
    "    max_value = np.max(data)\n",
    "    if max_indx >0:\n",
    "        for i in np.arange(max_indx):\n",
    "            valuehere = data[max_indx-i-1]\n",
    "            if valuehere < max_value/2:\n",
    "                break\n",
    "        lefthalf_indx = max_indx-i-1\n",
    "        # print(lefthalf_indx,valuehere,max_value/2)\n",
    "        # print(lefthalf_indx-1,data[lefthalf_indx-1])\n",
    "        # print(lefthalf_indx+1,data[lefthalf_indx+1])\n",
    "    else:\n",
    "        lefthalf_indx = 0\n",
    "    if len(data)-max_indx-1>0:\n",
    "        for i in np.arange(len(data)-max_indx):\n",
    "            valuehere = data[max_indx+i]\n",
    "            if valuehere < max_value/2:\n",
    "                break\n",
    "        righthalf_indx = max_indx+i\n",
    "    else:\n",
    "        righthalf_indx = len(data)-1\n",
    "\n",
    "    if lefthalf_indx == 0 and righthalf_indx!=len(data)-1:\n",
    "        lefthalf_indx = righthalf_indx\n",
    "        for i in np.arange(len(data)-1):\n",
    "            valuehere = data[len(data)-i-1]\n",
    "            if valuehere < max_value/2:\n",
    "                break\n",
    "        if i == 0: \n",
    "            righthalf_indx = len(data)-1\n",
    "        else:\n",
    "            righthalf_indx = len(data)-i-1\n",
    "        half_peak_width =len(data)-(righthalf_indx-lefthalf_indx)\n",
    "        left_half_real = righthalf_indx\n",
    "        right_half_real = lefthalf_indx\n",
    "    elif lefthalf_indx != 0 and righthalf_indx==len(data)-1:\n",
    "        righthalf_indx = lefthalf_indx\n",
    "        for i in np.arange(len(data)-1):\n",
    "            valuehere = data[i]\n",
    "            if valuehere < max_value/2:\n",
    "                break\n",
    "        lefthalf_indx = i\n",
    "        half_peak_width =len(data)-(righthalf_indx-lefthalf_indx)\n",
    "        left_half_real = righthalf_indx\n",
    "        right_half_real = lefthalf_indx\n",
    "    else:\n",
    "        # lefthalf_indx = lefthalf_indx\n",
    "        # righthalf_indx = righthalf_indx\n",
    "        half_peak_width = righthalf_indx-lefthalf_indx\n",
    "        left_half_real = lefthalf_indx\n",
    "        right_half_real = righthalf_indx\n",
    "    return half_peak_width,left_half_real,right_half_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define color for MP and SP group\n",
    "color1 = [254/255,129/255,126/255]\n",
    "color2 = [129/255,184/255,223/255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate effective dimension for 7 subgroups across different simulated sessions (Fig. 5C)\n",
    "non_singlelist = [54, 67, 84, 13, 4, 6, 88, 20, 97, 35, 83]\n",
    "non_selective_list = [12,42,44,45,51,68,69,71,79,82,94,95]\n",
    "singlelist = np.array(list(set(np.arange(100))-set(non_singlelist)-set(non_selective_list)))\n",
    "non_singlelist = np.array(non_singlelist)\n",
    "singlelist = singlelist.astype(int)\n",
    "non_singlelist = non_singlelist.astype(int)\n",
    "\n",
    "trials_all = np.arange(20)\n",
    "ed_unimodal_all = np.zeros((7,20))\n",
    "ed_multimodal_all = np.zeros(20)\n",
    "for trialnum in trials_all:\n",
    "    print(trialnum)\n",
    "    varname = '../simulated_data/trials_test_1000/output_simulated_200trials_1000t_epoch_500_testtrial_'+str(trialnum+1)+'_0609.mat'\n",
    "    data = sio.loadmat(varname)\n",
    "    data1 = data['data']\n",
    "    data1 = np.array(data1)\n",
    "    neuraldata = data1[0:100,:]\n",
    "    angle_list_target = data1[-3,:]\n",
    "    angle_list_rnnout = data1[-2,:]\n",
    "    angular_velocity = data1[-1,:]\n",
    "    data_single = neuraldata[singlelist,:]\n",
    "    non_singlelist = non_singlelist.astype(int)\n",
    "    data_nonsinglelist = neuraldata[non_singlelist,:]\n",
    "\n",
    "    half_peak_width_all = np.zeros(100)\n",
    "    for i in range(0,100):\n",
    "        firingdata = neuraldata[i,:]\n",
    "        tuning_i,theta = tuning_calculate(firingdata,angle_list_rnnout,divisions=50)\n",
    "        half_peak_width_i,left_half_index,right_half_index = fwhm_calculate(tuning_i)\n",
    "        half_peak_width_all[i] = half_peak_width_i\n",
    "\n",
    "    half_peak_width_all_new = half_peak_width_all[singlelist]\n",
    "    sorted_index = np.argsort(half_peak_width_all_new)\n",
    "    list_index = singlelist[sorted_index]\n",
    "    increase_width_group = list_index.reshape(7,11)\n",
    "    ed_all = np.zeros(7)\n",
    "    peak_width_tick = np.zeros(7)\n",
    "\n",
    "    for i in np.arange(7):\n",
    "        data = neuraldata[increase_width_group[i],:]\n",
    "        peak_width_tick[i] = np.mean(half_peak_width_all[increase_width_group[i]])/50*2*np.pi\n",
    "        # data = scipy.stats.zscore(data,axis=1)\n",
    "        data = data.T\n",
    "        pca = PCA()\n",
    "        _ = pca.fit(data)\n",
    "        explained = pca.explained_variance_ratio_\n",
    "        ed_all[i] = 1/np.sum(explained**2)\n",
    "    ed_unimodal_all[:,trialnum] = ed_all\n",
    "\n",
    "    data = neuraldata[non_singlelist,:]\n",
    "    data = data.T\n",
    "    pca = PCA()\n",
    "    _ = pca.fit(data)\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    ed_multimodal = 1/np.sum(explained**2)\n",
    "    ed_multimodal_all[trialnum] = ed_multimodal    \n",
    "\n",
    "ed_unimodal_mean = np.mean(ed_unimodal_all,axis=1)\n",
    "ed_unimodal_std = np.std(ed_unimodal_all,axis=1)\n",
    "ed_multimodal_mean = np.mean(ed_multimodal_all)\n",
    "ed_multimodal_std = np.std(ed_multimodal_all)\n",
    "\n",
    "# plot effective dimension\n",
    "x_index = np.arange(2,9)\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "ax = plt.subplot(111)\n",
    "_ = ax.plot(x_index,ed_unimodal_mean,'o-',color2,label='unimodal')\n",
    "_ = ax.fill_between(x_index,ed_unimodal_mean-ed_unimodal_std,ed_unimodal_mean+ed_unimodal_std,alpha=0.5,color='black')\n",
    "_ = ax.set_xticks(x_index)\n",
    "_ = ax.set_xticklabels(['{:.1f}'.format(xi) for xi in peak_width_tick])\n",
    "_ = ax.plot(1,ed_multimodal_mean,'o',color= color1,label='multimodal')\n",
    "_ = ax.errorbar(1,ed_multimodal_mean,yerr=ed_multimodal_std,color='red')\n",
    "_ = ax.set_xlabel('half peak width (rad)')\n",
    "_ = ax.set_ylabel('effective dimension')\n",
    "fig.savefig('../../figures/effective_dimension_unimodal_multimodal.pdf', bbox_inches='tight', transparent=True, format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural geometry-trajectory length analysis for 7 subgroups across different simulated sessions (Fig. 5C)\n",
    "divisions = 360\n",
    "dtheta = 1/divisions*360\n",
    "trials_all = np.arange(20)\n",
    "trajecotry_len_unimodal_all = np.zeros((7,20))\n",
    "trajecotry_len_multimodal_all = np.zeros(20)\n",
    "for trialnum in trials_all:\n",
    "    print(trialnum)\n",
    "    varname = '../simulated_data/trials_test_1000/output_simulated_200trials_1000t_epoch_500_testtrial_'+str(trialnum+1)+'_0609.mat'\n",
    "    data = sio.loadmat(varname)\n",
    "    data1 = data['data']\n",
    "    data1 = np.array(data1)\n",
    "    neuraldata = data1[0:100,:]\n",
    "    angle_list_target = data1[-3,:]\n",
    "    angle_list_rnnout = data1[-2,:]\n",
    "    angular_velocity = data1[-1,:]\n",
    "    down_ratio = 1\n",
    "    angle_list = angle_list_rnnout[::down_ratio]\n",
    "    #singlelist_rnn,singlelist_target,tuningfr_all_rnn,tuningfr_all_target = singlelistget(neuraldata,angle_list_rnnout,angle_list_target)\n",
    "    # singlelist = singlelist_rnn\n",
    "    data_single = neuraldata[singlelist,:]\n",
    "    non_singlelist = non_singlelist.astype(int)\n",
    "    data_nonsinglelist = neuraldata[non_singlelist,:]\n",
    "    # calculate length for unimodal groups\n",
    "    for groupnum in np.arange(7):\n",
    "        data = neuraldata[increase_width_group[groupnum],:]\n",
    "        data = scipy.stats.zscore(data,axis=1)\n",
    "        data = data[:,::down_ratio].T\n",
    "        average_manifold = np.zeros((divisions,np.shape(data)[1]))\n",
    "        for i in np.arange(0,divisions):\n",
    "            idx = (angle_list>i*dtheta)&(angle_list<(i+1)*dtheta)\n",
    "            average_manifold[i,:] = np.mean(data[idx,:],0)\n",
    "        trajecotry_len_unimodal_all[groupnum,trialnum] = np.sum(np.linalg.norm(np.diff(average_manifold,axis=0),axis=1))\n",
    "\n",
    "    data = neuraldata[non_singlelist,:]\n",
    "    data = scipy.stats.zscore(data,axis=1)\n",
    "    data = data[:,::down_ratio].T\n",
    "    average_manifold = np.zeros((divisions,np.shape(data)[1]))\n",
    "    for i in np.arange(0,divisions):\n",
    "        idx = (angle_list>i*dtheta)&(angle_list<(i+1)*dtheta)\n",
    "        average_manifold[i,:] = np.mean(data[idx,:],0)\n",
    "    trajecotry_len_multimodal_all[trialnum] = np.sum(np.linalg.norm(np.diff(average_manifold,axis=0),axis=1))\n",
    "\n",
    "trajecotry_len_unimodal_mean = np.mean(trajecotry_len_unimodal_all,axis=1)\n",
    "trajecotry_len_unimodal_std = np.std(trajecotry_len_unimodal_all,axis=1)\n",
    "trajecotry_len_multimodal_mean = np.mean(trajecotry_len_multimodal_all)\n",
    "trajecotry_len_multimodal_std = np.std(trajecotry_len_multimodal_all)\n",
    "\n",
    "x_index = np.arange(2,9)\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "ax = plt.subplot(111)\n",
    "_ = ax.plot(x_index,trajecotry_len_unimodal_mean ,'o-',color=color2, label='unimodal')\n",
    "_ = ax.fill_between(x_index,trajecotry_len_unimodal_mean-trajecotry_len_unimodal_std,trajecotry_len_unimodal_mean+trajecotry_len_unimodal_std,alpha=0.5,color= color2)\n",
    "_ = ax.set_xticks(x_index)\n",
    "_ = ax.set_xticklabels(['{:.1f}'.format(xi) for xi in peak_width_tick])\n",
    "_ = ax.plot(1,trajecotry_len_multimodal_mean,'o',color=color1,label='multimodal')\n",
    "_ = ax.errorbar(1,trajecotry_len_multimodal_mean,yerr=trajecotry_len_multimodal_std,color=color1)\n",
    "\n",
    "\n",
    "plt.hlines(trajecotry_len_multimodal_mean + trajecotry_len_multimodal_std, 1 - 0.2, 1 + 0.2, colors=color1)\n",
    "plt.hlines(trajecotry_len_multimodal_mean - trajecotry_len_multimodal_std, 1 - 0.2, 1 + 0.2, colors=color1)\n",
    "_ = ax.set_xlabel('half peak width (rad)')\n",
    "_ = ax.set_ylabel('Sum of trajectory length')\n",
    "plt.legend()\n",
    "fig.savefig('../../figures/trajectory_length_unimodal_multimodal.pdf', bbox_inches='tight', transparent=True, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example data\n",
    "non_singlelist = [54,67,84,13,4,6,88,20,97,35,83]\n",
    "non_selective_list = [12,42,44,45,51,68,69,71,79,82,94,95]\n",
    "singlelist = np.array(list(set(np.arange(100))-set(non_singlelist)-set(non_selective_list)))\n",
    "non_singlelist = np.array(non_singlelist)\n",
    "singlelist = singlelist.astype(int)\n",
    "non_singlelist = non_singlelist.astype(int)\n",
    "varname = '../simulated_data/epoch_test/output_simulated_200trials_1000t_epoch_'+str(500)+'_0228.mat'\n",
    "# varname = '../simulated_data/epoch_test/output_simulated_200trials_1000t_epoch_'+str(500)+'_0228.mat'\n",
    "data = sio.loadmat(varname)\n",
    "data1 = data['data']\n",
    "data1 = np.array(data1)\n",
    "neuraldata = data1[0:100,:]\n",
    "angle_list_target = data1[-3,:]\n",
    "angle_list_rnnout = data1[-2,:]\n",
    "angular_velocity = data1[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide SP units into 7 subgroups and PCA neural space visualization (Fig. 5D)\n",
    "with open('../../../results/All_metrics_individual_units_epoch500.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "half_peak_width_all = np.array(data['half_peak_width_all'])\n",
    "\n",
    "half_peak_width_all_new = half_peak_width_all[singlelist]\n",
    "sorted_index = np.argsort(half_peak_width_all_new)\n",
    "list_index = singlelist[sorted_index]\n",
    "increase_width_group = list_index.reshape(7,11)\n",
    "for groupi in range(7):\n",
    "    data = neuraldata[increase_width_group[groupi],:] # Notice: increase_width_group should be calculated also from the same file:str(500)+'_0228.mat\n",
    "    data = scipy.stats.zscore(data,axis=1)\n",
    "    down_ratio = 10\n",
    "    data = data[:,::down_ratio].T\n",
    "    pca = PCA(n_components=3)\n",
    "    x_embd = pca.fit_transform(data)\n",
    "    x_embd = 2*x_embd/np.max(np.abs(x_embd)) # normalize\n",
    "    axis_vectors = pca.components_\n",
    "    divisions = 360\n",
    "    dtheta = 1/divisions*360\n",
    "    angle_list = angle_list_rnnout[::down_ratio]\n",
    "    average_manifold = np.zeros((divisions,np.shape(data)[1]))\n",
    "    for i in np.arange(0,divisions):\n",
    "        idx = (angle_list>i*dtheta)&(angle_list<(i+1)*dtheta)\n",
    "        average_manifold[i,:] = np.mean(data[idx,:],0)\n",
    "    projections = np.matmul(average_manifold,axis_vectors.T)\n",
    "    projections_new = projections*1\n",
    "    %matplotlib widget\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.set_cmap('hsv') # circular cmap\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.azim = -12.66 # finally use this angle to visualize\n",
    "    ax.elev = 19.18\n",
    "    ds_plt = 3\n",
    "    cmap = angle_list[::ds_plt]\n",
    "    scat = ax.scatter(x_embd[:,0][::ds_plt], x_embd[:,1][::ds_plt], x_embd[:,2][::ds_plt], c=cmap, alpha=.7)\n",
    "    cbar = plt.colorbar(scat)\n",
    "    cbar.set_label('HD')\n",
    "    ax.plot(projections_new[:,0],projections_new[:,1],projections_new[:,2],c='k')\n",
    "    cmap = np.arange(0,divisions)\n",
    "    ax.scatter(projections_new[:,0],projections_new[:,1],projections_new[:,2],c=cmap,alpha =.5)\n",
    "    fig.savefig('../../figures/low_d_manifold_uni_inc_group_IV.pdf', bbox_inches='tight', transparent=True, format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for population ANN decoding\n",
    "class VelocityNet(nn.Module):\n",
    "    def __init__(self, in_features) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(in_features, 64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64, 128),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(128, 64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64, 2))\n",
    "\n",
    "        # self.layers = nn.Sequential(nn.Linear(in_features, 64),\n",
    "        #                             nn.ReLU(),\n",
    "        #                             nn.Linear(64, 64),\n",
    "        #                             nn.ReLU(),\n",
    "        #                             nn.Linear(64, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        v_hat = y[:, 0]\n",
    "        v_var = softplus(y[:, 1])\n",
    "        # v_var = torch.ones_like(v_var)\n",
    "        return v_hat, v_var\n",
    "\n",
    "\n",
    "class AngleNet(nn.Module):\n",
    "    def __init__(self, in_features) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(in_features, 64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64, 128),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(128, 64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64, 3))\n",
    "        # self.layers = nn.Sequential(nn.Linear(in_features, 64),\n",
    "        #                             nn.Tanh(),\n",
    "        #                             nn.Linear(64, 64),\n",
    "        #                             nn.Tanh(),\n",
    "        #                             nn.Linear(64, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        a = y[:, [0, 1]]\n",
    "        a = a / (torch.sqrt(a.pow(2).sum(-1, keepdim=True)) + 1e-8)\n",
    "        a_cos = a[:, 0]\n",
    "        a_sin = a[:, 1]\n",
    "        # a_kap = softplus(y[:, 2])\n",
    "        # with torch.no_grad():\n",
    "        #     a_kap.clamp_max_(10)\n",
    "        # a_kap = a_kap.clamp_max_(10)\n",
    "        a_kap = 10*torch.sigmoid(y[:, 2])\n",
    "        return a_cos, a_sin, a_kap\n",
    "    \n",
    "\n",
    "def make_dataloader(x, y, device=None):\n",
    "    x = torch.Tensor(x).to(device)\n",
    "    y = torch.Tensor(y).to(device)\n",
    "    data_set = Data.TensorDataset(x, y)\n",
    "    data_loader = Data.DataLoader(dataset=data_set,\n",
    "                                  batch_size=512,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def make_optim(net, weight_dacay=1e-2):\n",
    "    return Adam(net.parameters(), weight_decay=weight_dacay)\n",
    "\n",
    "\n",
    "def make_vloss():\n",
    "    return nn.GaussianNLLLoss()\n",
    "\n",
    "\n",
    "def make_aloss():\n",
    "    from torch.special import i0\n",
    "\n",
    "    def von_Mises_loss(a_cos_hat, a_sin_hat, angle, kappa):\n",
    "        a_cos = torch.cos(angle)\n",
    "        a_sin = torch.sin(angle)\n",
    "        loss = -(kappa * (a_cos * a_cos_hat + a_sin *\n",
    "                 a_sin_hat) - torch.log(i0(kappa))).mean()\n",
    "        return loss\n",
    "    return von_Mises_loss\n",
    "\n",
    "\n",
    "def cal_angle_tderiv(rates, rates_tderiv, angle_net, optim):\n",
    "    rates = rates.clone().detach().requires_grad_(True)\n",
    "    a_cos_hat, a_sin_hat, _ = angle_net(rates)\n",
    "    a_hat = torch.arctan2(a_sin_hat, a_cos_hat)\n",
    "    optim.zero_grad()\n",
    "    a_hat.backward(torch.ones_like(a_hat))\n",
    "    optim.zero_grad()\n",
    "    angle_tderiv = (rates.grad * rates_tderiv).sum(-1).numpy() / 50\n",
    "    return angle_tderiv\n",
    "\n",
    "def cum(a, window=9):\n",
    "    b = a.cumsum()\n",
    "    b[window:] = b[window:] - b[:-window]\n",
    "    return b\n",
    "\n",
    "\n",
    "def train_and_test(session,data,nmin):\n",
    "    # rates, rates_tderiv, velocity, angle = data['rates'], data['rates t-deriv'], data['velocity'], data['angle']\n",
    "    sinlgerates, nonsinglerates,velocity, angle = data['single_rates'],data['nonsingle_rates'],data['velocity'], data['angle']\n",
    "    r1 = r2 = r3 = r4 = 0\n",
    "    infeature = np.shape(sinlgerates)\n",
    "    # print('infeature single', infeature)\n",
    "    rates = sinlgerates\n",
    "    if infeature[0]==0:\n",
    "        infeature = 0\n",
    "    else:\n",
    "        infeature = infeature[1]\n",
    "    if infeature >=nmin:\n",
    "        temp  = int(np.floor(len(rates)/4))\n",
    "        # print(np.shape(rates[temp:]))\n",
    "        dl_a = make_dataloader(rates[temp:], angle[temp:])\n",
    "        a_net = AngleNet(in_features=infeature)\n",
    "        optim_a = make_optim(a_net, weight_dacay=5e-2)\n",
    "        loss_a = make_aloss()\n",
    "        epochs = 10\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (r, a) in enumerate(dl_a):\n",
    "                with autograd.set_detect_anomaly(False):\n",
    "                    a_cos_hat, a_sin_hat, a_kap = a_net(r)\n",
    "                    batch_loss = loss_a(a_cos_hat, a_sin_hat, a, a_kap)\n",
    "                    optim_a.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    optim_a.step()\n",
    "                    epoch_loss.append(batch_loss)\n",
    "\n",
    "        dl_v = make_dataloader(rates[temp:], velocity[temp:])\n",
    "        v_net = VelocityNet(in_features=infeature)\n",
    "        optim_v = make_optim(v_net, 5e-3)\n",
    "        loss_v = make_vloss()\n",
    "        epochs = 20\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (r, v) in enumerate(dl_v):\n",
    "                v_hat, v_var = v_net(r)\n",
    "                batch_loss = loss_v(v_hat, v, v_var)\n",
    "                optim_v.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optim_v.step()\n",
    "                epoch_loss.append(batch_loss)           \n",
    "        angle_sample = angle[:temp]\n",
    "        rates_sample = torch.Tensor(rates[:temp])\n",
    "        a_cos_hat, a_sin_hat, _ = a_net(rates_sample)\n",
    "        a_cos_hat, a_sin_hat = a_cos_hat.detach().numpy(), a_sin_hat.detach().numpy()\n",
    "        r1 = np.corrcoef(np.cos(angle_sample), a_cos_hat)\n",
    "        \n",
    "        r1 = r1[0,1]\n",
    "        velocity_sample = velocity[:temp]\n",
    "        rates_sample = torch.Tensor(rates[:temp])\n",
    "        velocity_hat, _ = v_net(rates_sample)\n",
    "        velocity_hat = velocity_hat.detach().numpy()\n",
    "        r2 = np.corrcoef(velocity_sample, velocity_hat)\n",
    "        r2 = r2[0,1]\n",
    "\n",
    "    rates = nonsinglerates\n",
    "    infeature = np.shape(rates)\n",
    "    if infeature[0]==0:\n",
    "        infeature = 0\n",
    "    else:\n",
    "        infeature = infeature[1]\n",
    "    if infeature >=nmin:\n",
    "        temp  = int(np.floor(len(rates)/4))\n",
    "        dl_a = make_dataloader(rates[temp:], angle[temp:])\n",
    "        a_net = AngleNet(in_features=infeature)\n",
    "        optim_a = make_optim(a_net, weight_dacay=5e-2)\n",
    "        loss_a = make_aloss()\n",
    "        epochs = 10\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (r, a) in enumerate(dl_a):\n",
    "                with autograd.set_detect_anomaly(False):\n",
    "                    a_cos_hat, a_sin_hat, a_kap = a_net(r)\n",
    "                    batch_loss = loss_a(a_cos_hat, a_sin_hat, a, a_kap)\n",
    "                    optim_a.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    optim_a.step()\n",
    "                    epoch_loss.append(batch_loss)\n",
    "\n",
    "        dl_v = make_dataloader(rates[temp:], velocity[temp:])\n",
    "        v_net = VelocityNet(in_features=infeature)\n",
    "        optim_v = make_optim(v_net, 5e-3)\n",
    "        loss_v = make_vloss()\n",
    "        epochs = 20\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (r, v) in enumerate(dl_v):\n",
    "                v_hat, v_var = v_net(r)\n",
    "                batch_loss = loss_v(v_hat, v, v_var)\n",
    "                optim_v.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optim_v.step()\n",
    "                epoch_loss.append(batch_loss)           \n",
    "        angle_sample = angle[:temp]\n",
    "        rates_sample = torch.Tensor(rates[:temp])\n",
    "        a_cos_hat, a_sin_hat, _ = a_net(rates_sample)\n",
    "        a_cos_hat, a_sin_hat = a_cos_hat.detach().numpy(), a_sin_hat.detach().numpy()\n",
    "        r3 = np.corrcoef(np.cos(angle_sample), a_cos_hat)\n",
    "        r3 = r3[0,1]\n",
    "        velocity_sample = velocity[:temp]\n",
    "        rates_sample = torch.Tensor(rates[:temp])\n",
    "        velocity_hat, _ = v_net(rates_sample)\n",
    "        velocity_hat = velocity_hat.detach().numpy()\n",
    "        r4 = np.corrcoef(velocity_sample, velocity_hat)\n",
    "        r4 = r4[0,1]        \n",
    "    return r1,r2,r3,r4\n",
    "\n",
    "\n",
    "def get_cor(rate,angle,velocity):\n",
    "    numT = rate.shape[0]\n",
    "    train_size = numT * 0.8\n",
    "    test_size = numT - train_size\n",
    "    r1_all = []\n",
    "    r2_all = []\n",
    "    for i in np.arange(5):        \n",
    "        infeature = rate.shape[1]\n",
    "        test_index = list(range(int(i*test_size),int(i*test_size+test_size)))\n",
    "        train_index = list(range(0,int(i*test_size)))+list(range(int(i*test_size+test_size),numT))\n",
    "        dl_a = make_dataloader(rate[train_index], angle[train_index])\n",
    "        a_net = AngleNet(in_features=infeature)\n",
    "        optim_a = make_optim(a_net, weight_dacay=5e-2)\n",
    "        loss_a = make_aloss()\n",
    "        epochs = 10\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (r, a) in enumerate(dl_a):\n",
    "                with autograd.set_detect_anomaly(False):\n",
    "                    a_cos_hat, a_sin_hat, a_kap = a_net(r)\n",
    "                    batch_loss = loss_a(a_cos_hat, a_sin_hat, a, a_kap)\n",
    "                    optim_a.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    optim_a.step()\n",
    "                    epoch_loss.append(batch_loss)\n",
    "\n",
    "        dl_v = make_dataloader(rate[train_index], velocity[train_index])\n",
    "        v_net = VelocityNet(in_features=infeature)\n",
    "        optim_v = make_optim(v_net, 5e-3)\n",
    "        loss_v = make_vloss()\n",
    "        epochs = 20\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (r, v) in enumerate(dl_v):\n",
    "                v_hat, v_var = v_net(r)\n",
    "                batch_loss = loss_v(v_hat, v, v_var)\n",
    "                optim_v.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optim_v.step()\n",
    "                epoch_loss.append(batch_loss)    \n",
    "\n",
    "        angle_sample = angle[test_index]\n",
    "        rates_sample = torch.Tensor(rate[test_index])\n",
    "        a_cos_hat, a_sin_hat, _ = a_net(rates_sample)\n",
    "        a_cos_hat, a_sin_hat = a_cos_hat.detach().numpy(), a_sin_hat.detach().numpy()\n",
    "        r1 = np.corrcoef(np.cos(angle_sample), a_cos_hat)\n",
    "        r1 = r1[0,1]\n",
    "        r1_all.append(r1)\n",
    "        \n",
    "        velocity_sample = velocity[test_index]\n",
    "        rates_sample = torch.Tensor(rate[test_index])\n",
    "        velocity_hat, _ = v_net(rates_sample)\n",
    "        velocity_hat = velocity_hat.detach().numpy()\n",
    "        r2 = np.corrcoef(velocity_sample, velocity_hat)\n",
    "        r2 = r2[0,1]\n",
    "        r2_all.append(r2)\n",
    "    # r_angle = np.mean(r1_all)\n",
    "    # r_velocity = np.mean(r2_all)\n",
    "    return r1_all, r2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example decoding results for HD/AHV for SP/MP populations （Fig. 3A）\n",
    "# load example data\n",
    "non_singlelist = [54,67,84,13,4,6,88,20,97,35,83]\n",
    "non_selective_list = [12,42,44,45,51,68,69,71,79,82,94,95]\n",
    "singlelist = np.array(list(set(np.arange(100))-set(non_singlelist)-set(non_selective_list)))\n",
    "non_singlelist = np.array(non_singlelist)\n",
    "singlelist = singlelist.astype(int)\n",
    "non_singlelist = non_singlelist.astype(int)\n",
    "varname = '../simulated_data/epoch_test/output_simulated_200trials_1000t_epoch_'+str(500)+'_0228.mat'\n",
    "# varname = '../simulated_data/epoch_test/output_simulated_200trials_1000t_epoch_'+str(500)+'_0228.mat'\n",
    "data = sio.loadmat(varname)\n",
    "data1 = data['data']\n",
    "data1 = np.array(data1)\n",
    "neuraldata = data1[0:100,:]\n",
    "angle_list_target = data1[-3,:]\n",
    "angle_list_rnnout = data1[-2,:]\n",
    "angular_velocity = data1[-1,:]\n",
    "data_single = neuraldata[singlelist,:]\n",
    "data_nonsinglelist = neuraldata[non_singlelist,:]\n",
    "rangeuse = np.arange(100000)\n",
    "\n",
    "# for SP units, decode HD and AHV\n",
    "data = data_single[:,rangeuse]\n",
    "rates = data.T\n",
    "rates = rates-rates.mean()\n",
    "rates = rates/rates.std()\n",
    "angle = angle_list_rnnout[rangeuse]/180*np.pi\n",
    "print('angle_min_max',np.min(angle), np.max(angle))\n",
    "infeature = np.shape(rates)\n",
    "if infeature[0]==0:\n",
    "    infeature = 0\n",
    "else:\n",
    "    infeature = infeature[1]\n",
    "temp  = int(np.floor(len(rates)/5))\n",
    "dl_a = make_dataloader(rates[temp:], angle[temp:])\n",
    "a_net = AngleNet(in_features=infeature)\n",
    "optim_a = make_optim(a_net, weight_dacay=5e-2)\n",
    "loss_a = make_aloss()\n",
    "epochs = 10\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for batch, (r, a) in enumerate(dl_a):\n",
    "        with autograd.set_detect_anomaly(False):\n",
    "            a_cos_hat, a_sin_hat, a_kap = a_net(r)\n",
    "            batch_loss = loss_a(a_cos_hat, a_sin_hat, a, a_kap)\n",
    "            optim_a.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optim_a.step()\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "\n",
    "angular_velocity_use = angular_velocity[rangeuse]\n",
    "dl_v = make_dataloader(rates[temp:], angular_velocity_use[temp:])\n",
    "v_net = VelocityNet(in_features=infeature)\n",
    "optim_v = make_optim(v_net, 5e-3)\n",
    "loss_v = make_vloss()\n",
    "epochs = 20\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for batch, (r, v) in enumerate(dl_v):\n",
    "        with autograd.set_detect_anomaly(False):\n",
    "            v_hat, v_var = v_net(r)\n",
    "            batch_loss = loss_v(v_hat, v, v_var)\n",
    "            optim_v.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optim_v.step()\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "angle_sample = angle[:temp]\n",
    "rates_sample = torch.Tensor(rates[:temp])\n",
    "a_cos_hat, a_sin_hat, _ = a_net(rates_sample)\n",
    "a_cos_hat, a_sin_hat = a_cos_hat.detach().numpy(), a_sin_hat.detach().numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "x = angle_sample\n",
    "y = np.arctan2(a_sin_hat, a_cos_hat) % (2*np.pi)\n",
    "data = {'angle_sample':x, 'angle_predicted':y}\n",
    "g = sns.jointplot(x='angle_sample', y='angle_predicted', data=data, marginal_kws=dict(bins=20, color=color2, alpha=0.5))\n",
    "g.ax_joint.clear()\n",
    "g.plot_joint(sns.scatterplot, color= color2, s=20, alpha=0.5)   # 绘制散点图\n",
    "_ = sns.regplot(x='angle_sample', y='angle_predicted', data=data, ax=g.ax_joint, scatter=False, color=color2, line_kws={'linestyle': '--'})  # 绘制回归曲线\n",
    "ax = plt.gca()\n",
    "_ = ax.set_title('unimodal')\n",
    "_ = ax.set_ylabel('HD_predicted')\n",
    "_ = ax.set_xlabel('HD')\n",
    "_ = ax.title.set_position([1.15, 1.15])\n",
    "g.savefig('../../figures/representative_scaatter_plot_angle_r_unimodal.pdf', bbox_inches='tight', transparent=True, format='pdf')\n",
    "\n",
    "velocity_sample = angular_velocity_use[:temp]\n",
    "rates_sample = torch.Tensor(rates[:temp])\n",
    "velocity_hat, _ = v_net(rates_sample)\n",
    "velocity_hat = velocity_hat.detach().numpy()\n",
    "r2 = np.corrcoef(velocity_sample, velocity_hat)\n",
    "fig = plt.figure()\n",
    "data = {'velocity_sample':velocity_sample, 'velocity_predicted':velocity_hat}\n",
    "g = sns.jointplot(x='velocity_sample', y='velocity_predicted', data=data, marginal_kws=dict(bins=20, color= color2, alpha=0.5))\n",
    "g.ax_joint.clear()\n",
    "g.plot_joint(sns.scatterplot, color=color2, s=20, alpha=0.5)   # 绘制散点图\n",
    "_ = sns.regplot(x='velocity_sample', y='velocity_predicted', data=data, ax=g.ax_joint, scatter=False, color=color2, line_kws={'linestyle': '--'})  # 绘制回归曲线\n",
    "ax = plt.gca()\n",
    "_ = ax.set_title('unimodal')\n",
    "_ = ax.title.set_position([1.15, 1.15])\n",
    "_ = ax.set_ylabel('AHV_predicted')\n",
    "_ = ax.set_xlabel('AHV')\n",
    "g.savefig('../../figures/representative_scaatter_plot_AHV_r_unimodal.pdf', bbox_inches='tight', transparent=True, format='pdf')\n",
    "\n",
    "# for MP units, decode HD and AHV\n",
    "data = data_nonsinglelist[:,rangeuse]\n",
    "rates = data.T\n",
    "rates = rates-rates.mean()\n",
    "rates = rates/rates.std()\n",
    "angle = angle_list_rnnout[rangeuse]/180*np.pi\n",
    "print('angle_min_max',np.min(angle), np.max(angle))\n",
    "infeature = np.shape(rates)\n",
    "if infeature[0]==0:\n",
    "    infeature = 0\n",
    "else:\n",
    "    infeature = infeature[1]\n",
    "temp  = int(np.floor(len(rates)/5))\n",
    "dl_a = make_dataloader(rates[temp:], angle[temp:])\n",
    "a_net = AngleNet(in_features=infeature)\n",
    "optim_a = make_optim(a_net, weight_dacay=5e-2)\n",
    "loss_a = make_aloss()\n",
    "epochs = 10\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for batch, (r, a) in enumerate(dl_a):\n",
    "        with autograd.set_detect_anomaly(False):\n",
    "            a_cos_hat, a_sin_hat, a_kap = a_net(r)\n",
    "            batch_loss = loss_a(a_cos_hat, a_sin_hat, a, a_kap)\n",
    "            optim_a.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optim_a.step()\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "\n",
    "angular_velocity_use = angular_velocity[rangeuse]\n",
    "dl_v = make_dataloader(rates[temp:], angular_velocity_use[temp:])\n",
    "v_net = VelocityNet(in_features=infeature)\n",
    "optim_v = make_optim(v_net, 5e-3)\n",
    "loss_v = make_vloss()\n",
    "epochs = 20\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for batch, (r, v) in enumerate(dl_v):\n",
    "        with autograd.set_detect_anomaly(False):\n",
    "            v_hat, v_var = v_net(r)\n",
    "            batch_loss = loss_v(v_hat, v, v_var)\n",
    "            optim_v.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optim_v.step()\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "angle_sample = angle[:temp]\n",
    "angle_sample2 = angle[temp:2*temp]\n",
    "rates_sample = torch.Tensor(rates[:temp])\n",
    "rates_sample2 = torch.Tensor(rates[temp:2*temp])\n",
    "a_cos_hat, a_sin_hat, _ = a_net(rates_sample)\n",
    "a_cos_hat2, a_sin_hat2, _ = a_net(rates_sample2)\n",
    "a_cos_hat, a_sin_hat = a_cos_hat.detach().numpy(), a_sin_hat.detach().numpy()\n",
    "a_cos_hat2, a_sin_hat2 = a_cos_hat2.detach().numpy(), a_sin_hat2.detach().numpy()\n",
    "\n",
    "x = angle_sample\n",
    "y = np.arctan2(a_sin_hat, a_cos_hat) % (2*np.pi)\n",
    "\n",
    "data = {'angle_sample':x, 'angle_predicted':y}\n",
    "_ = g = sns.jointplot(x='angle_sample', y='angle_predicted', data=data, marginal_kws=dict(bins=20, color='b', alpha=0.5))\n",
    "_ = g.ax_joint.clear()\n",
    "_ = g.plot_joint(sns.scatterplot, color=color2, s=5, alpha=0.5)   # 绘制散点图\n",
    "_ = sns.regplot(x='angle_sample', y='angle_predicted', data=data, ax=g.ax_joint, scatter=False, color='b', line_kws={'linestyle': '--'})  # 绘制回归曲线\n",
    "ax = plt.gca()\n",
    "_ = ax.set_title('multimodal')\n",
    "_ = ax.set_ylabel('HD_predicted_on_testing_dta')\n",
    "_ = ax.set_xlabel('HD')\n",
    "_ = ax.title.set_position([1.2, 1.2])\n",
    "# fig.savefig('../../figures/representative_scaatter_plot_angle_r_unimodal.pdf', bbox_inches='tight', transparent=True, format='pdf')\n",
    "\n",
    "\n",
    "x = angle_sample2\n",
    "y = np.arctan2(a_sin_hat2, a_cos_hat2) % (2*np.pi)\n",
    "residual = y - x\n",
    "indx = np.where(residual>np.pi)[0]\n",
    "if len(indx) > 0:\n",
    "    indx1 = np.random.choice(indx, int(len(indx)*0.5), replace=False)\n",
    "    indx2 = np.setdiff1d(indx, indx1)\n",
    "    x[indx1] = x[indx1] + 2*np.pi\n",
    "    y[indx2] = y[indx2] - 2*np.pi\n",
    "\n",
    "indx = np.where(residual<-np.pi)[0]\n",
    "if len(indx) > 0:\n",
    "    indx1 = np.random.choice(indx, int(len(indx)*0.5), replace=False)\n",
    "    indx2 = np.setdiff1d(indx, indx1)\n",
    "    x[indx1] = x[indx1] - 2*np.pi\n",
    "    y[indx2] = y[indx2] + 2*np.pi\n",
    "data = {'angle_sample':x, 'angle_predicted':y}\n",
    "_ = g = sns.jointplot(x='angle_sample', y='angle_predicted', data=data, marginal_kws=dict(bins=20, color='b', alpha=0.5))\n",
    "_ = g.ax_joint.clear()\n",
    "_ = g.plot_joint(sns.scatterplot, color='b', s=5, alpha=0.5)   # 绘制散点图\n",
    "_ = sns.regplot(x='angle_sample', y='angle_predicted', data=data, ax=g.ax_joint, scatter=False, color='b', line_kws={'linestyle': '--'})  # 绘制回归曲线\n",
    "ax = plt.gca()\n",
    "_ = ax.set_title('multimodal')\n",
    "_ = ax.set_ylabel('HD_predicted_on_training_data')\n",
    "_ = ax.set_xlabel('HD')\n",
    "_ = ax.title.set_position([1.2, 1.2])\n",
    "velocity_sample = angular_velocity_use[:temp]\n",
    "velocity_sample2 = angular_velocity_use[temp:2*temp]\n",
    "rates_sample = torch.Tensor(rates[:temp])\n",
    "rates_sample2 = torch.Tensor(rates[temp:2*temp])\n",
    "velocity_hat, _ = v_net(rates_sample)\n",
    "velocity_hat2, _ = v_net(rates_sample2)\n",
    "velocity_hat = velocity_hat.detach().numpy()\n",
    "velocity_hat2 = velocity_hat2.detach().numpy()\n",
    "r2 = np.corrcoef(velocity_sample, velocity_hat)\n",
    "fig = plt.figure()\n",
    "data = {'velocity_sample':velocity_sample, 'velocity_predicted':velocity_hat}\n",
    "g = sns.jointplot(x='velocity_sample', y='velocity_predicted', data=data, marginal_kws=dict(bins=20, color='b', alpha=0.5))\n",
    "g.ax_joint.clear()\n",
    "g.plot_joint(sns.scatterplot, color='b', s=5, alpha=0.5)   # 绘制散点图\n",
    "sns.regplot(x='velocity_sample', y='velocity_predicted', data=data, ax=g.ax_joint, scatter=False, color='b', line_kws={'linestyle': '--'})  # 绘制回归曲线\n",
    "ax = plt.gca()\n",
    "_ = ax.set_title('multimodal')\n",
    "_ = ax.title.set_position([1.2, 1.2])\n",
    "_ = ax.set_ylabel('AHV_predicted_on_testing_data')\n",
    "_ = ax.set_xlabel('AHV')\n",
    "# fig.savefig('../../figures/representative_scaatter_plot_AHV_r_unimodal.pdf', bbox_inches='tight', transparent=True, format='pdf')#\n",
    "fig = plt.figure()\n",
    "data = {'velocity_sample':velocity_sample2, 'velocity_predicted':velocity_hat2}\n",
    "g = sns.jointplot(x='velocity_sample', y='velocity_predicted', data=data, marginal_kws=dict(bins=20, color='b', alpha=0.5))\n",
    "g.ax_joint.clear()\n",
    "g.plot_joint(sns.scatterplot, color='b', s=5, alpha=0.5)   # 绘制散点图\n",
    "_ = sns.regplot(x='velocity_sample', y='velocity_predicted', data=data, ax=g.ax_joint, scatter=False, color='b', line_kws={'linestyle': '--'})  # 绘制回归曲线\n",
    "ax = plt.gca()\n",
    "_ = ax.set_title('multimodal')\n",
    "_ = ax.title.set_position([1.2, 1.2])\n",
    "_ = ax.set_ylabel('AHV_predicted_on_training_data')\n",
    "_ = ax.set_xlabel('AHV')\n",
    "\n",
    "\n",
    "# 5-fold correlation calculation and average\n",
    "for iterationn in [1,2]:   \n",
    "    if iterationn == 1:\n",
    "        data = data_single[:,rangeuse]\n",
    "    else:\n",
    "        data = data_nonsinglelist[:,rangeuse]    \n",
    "    rates = data.T\n",
    "    rates = rates-rates.mean()\n",
    "    rates = rates/rates.std()\n",
    "    angle = angle_list_rnnout[rangeuse]/180*np.pi\n",
    "    r_angle_5_fold, r_v_5_fold = get_cor(rates,angle,angular_velocity)\n",
    "    if iterationn == 1:\n",
    "        print('r for single_peak cells are',[r_angle_5_fold, r_v_5_fold])\n",
    "        r_single = [r_angle_5_fold, r_v_5_fold]\n",
    "    else:\n",
    "        print('r for non_single_peak cells are',[r_angle_5_fold, r_v_5_fold])\n",
    "        r_non_single = [r_angle_5_fold, r_v_5_fold]\n",
    "# get mean and std\n",
    "r_single = np.array(r_single)\n",
    "r_non_single = np.array(r_non_single)\n",
    "\n",
    "r_angle_single = np.mean(r_single[0,:])\n",
    "r_v_single = np.mean(r_single[1,:])\n",
    "r_angle_non_single = np.mean(r_non_single[0,:])\n",
    "r_v_non_single = np.mean(r_non_single[1,:])\n",
    "\n",
    "print('r for single_peak cells are',[r_angle_single, r_v_single])\n",
    "print('r for non_single_peak cells are',[r_angle_non_single, r_v_non_single])\n",
    "\n",
    "r_angle_single_std = np.std(r_single[0,:])\n",
    "r_v_single_std = np.std(r_single[1,:])\n",
    "r_angle_non_single_std = np.std(r_non_single[0,:])\n",
    "r_v_non_single_std = np.std(r_non_single[1,:])\n",
    "\n",
    "print('r-std for single_peak cells are',[r_angle_single_std, r_v_single_std])\n",
    "print('r-std for non_single_peak cells are',[r_angle_non_single_std, r_v_non_single_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding correlations across 50 simulated sessions (Fig. 3C)\n",
    "trials_all = np.arange(50)\n",
    "r_a_single = np.zeros_like(trials_all,dtype=float)\n",
    "r_v_single = np.zeros_like(trials_all,dtype=float)\n",
    "r_a_non_single = np.zeros_like(trials_all,dtype=float)\n",
    "r_v_non_single = np.zeros_like(trials_all,dtype=float)\n",
    "r_a_single_std = np.zeros_like(trials_all,dtype=float)\n",
    "r_v_single_std = np.zeros_like(trials_all,dtype=float)\n",
    "r_a_non_single_std = np.zeros_like(trials_all,dtype=float)\n",
    "r_v_non_single_std = np.zeros_like(trials_all,dtype=float)\n",
    "\n",
    "for i,trialnum in enumerate(trials_all):\n",
    "    print(i)\n",
    "    varname = '../simulated_data/trials_test_1000/output_simulated_200trials_1000t_epoch_500_testtrial_'+str(trialnum+1)+'_0609.mat'\n",
    "    data = sio.loadmat(varname)\n",
    "    data1 = data['data']\n",
    "    data1 = np.array(data1)\n",
    "    neuraldata = data1[0:100,:]\n",
    "    angle_list_target = data1[-3,:]\n",
    "    angle_list_rnnout = data1[-2,:]\n",
    "    angular_velocity = data1[-1,:]\n",
    "    data_single = neuraldata[singlelist,:]\n",
    "    non_singlelist = non_singlelist.astype(int)\n",
    "    data_nonsinglelist = neuraldata[non_singlelist,:]\n",
    "\n",
    "    rangeuse = np.arange(100000)\n",
    "    for iterationn in [1,2]:   \n",
    "        if iterationn == 1:\n",
    "            data = data_single[:,rangeuse]\n",
    "        else:\n",
    "            data = data_nonsinglelist[:,rangeuse]    \n",
    "        rates = data.T\n",
    "        rates = rates-rates.mean()\n",
    "        rates = rates/rates.std()\n",
    "        angle = angle_list_rnnout[rangeuse]/180*np.pi\n",
    "        r_angle_5_fold, r_v_5_fold = get_cor(rates,angle,angular_velocity)\n",
    "        if iterationn == 1:\n",
    "            r_a_single[i] = np.mean(r_angle_5_fold)\n",
    "            r_v_single[i] = np.mean(r_v_5_fold)\n",
    "            # print('r_v_single[i]',r_v_5_fold)\n",
    "            r_a_single_std[i] = np.std(r_angle_5_fold)\n",
    "            r_v_single_std[i] = np.std(r_v_5_fold)\n",
    "        else:\n",
    "            r_a_non_single[i] = np.mean(r_angle_5_fold)\n",
    "            # print('r_v_non_single[i]',r_v_5_fold)\n",
    "            r_v_non_single[i] = np.mean(r_v_5_fold)\n",
    "            r_a_non_single_std[i] = np.std(r_angle_5_fold)\n",
    "            r_v_non_single_std[i] = np.std(r_v_5_fold)\n",
    "\n",
    "\n",
    "import pickle\n",
    "data = {'r_a_single':r_a_single,'r_v_single':r_v_single,'r_a_non_single':r_a_non_single,\n",
    "        'r_v_non_single':r_v_non_single,'r_a_single_std':r_a_single_std,'r_v_single_std':r_v_single_std,\n",
    "        'r_a_non_single_std':r_a_non_single_std,'r_v_non_single_std':r_v_non_single_std}\n",
    "\n",
    "with open('../../results/trials_test_50_traindatasize_100000_all_r_save.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "import pickle\n",
    "with open('../../results/trials_test_50_traindatasize_100000_all_r_save.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "r_a_single = data['r_a_single']\n",
    "r_v_single = data['r_v_single']\n",
    "r_a_non_single = data['r_a_non_single']\n",
    "r_v_non_single = data['r_v_non_single']\n",
    "r_a_single_std = data['r_a_single_std']\n",
    "r_v_single_std = data['r_v_single_std']\n",
    "r_a_non_single_std = data['r_a_non_single_std']\n",
    "r_v_non_single_std = data['r_v_non_single_std']\n",
    "r_a_single = r_a_single\n",
    "r_v_single = r_v_single\n",
    "r_a_non_single = r_a_non_single\n",
    "r_v_non_single = r_v_non_single\n",
    "r_a_single_std = r_a_single_std\n",
    "r_v_single_std = r_v_single_std\n",
    "r_a_non_single_std = r_a_non_single_std\n",
    "r_v_non_single_std = r_v_non_single_std\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.boxplot([r_a_single, r_a_non_single,r_v_single, r_v_non_single], positions = [1,2,3,4], widths = 0.3, showfliers=False)\n",
    "_ = plt.scatter(np.ones(len(r_a_single))+ np.random.randn(len(r_a_single)) * 0.1, r_a_single, s = 30, marker='o', c=color2,alpha=0.5)\n",
    "_ = plt.scatter(np.ones(len(r_a_non_single))*2+ np.random.randn(len(r_a_non_single)) * 0.1, r_a_non_single, s = 30,marker='o', c=color1,alpha=0.5)\n",
    "_ = plt.scatter(np.ones(len(r_v_single))*3+ np.random.randn(len(r_v_single)) * 0.1, r_v_single, color = color2, s = 30, marker='o',facecolor='none', edgecolor=color2, alpha=0.5)\n",
    "_ = plt.scatter(np.ones(len(r_v_non_single))*4+ np.random.randn(len(r_v_non_single)) * 0.1, r_v_non_single, color = color1, s = 30,marker='o', facecolor='none', edgecolor=color1, alpha=0.5)\n",
    "ax = plt.gca()\n",
    "fig = plt.gcf()\n",
    "ax.set_xticks([1.5,3.5])\n",
    "ax.set_xticklabels(['Angle decoding', 'Velocity decoding'])\n",
    "ax.set_ylabel('Pearson''s Correlation coefficient')\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(r_a_single, r_a_non_single)\n",
    "print('r_a_single vs r_a_non_single: p = ', p)\n",
    "stat, p = mannwhitneyu(r_v_single, r_v_non_single)\n",
    "print('r_v_single vs r_v_non_single: p = ', p)\n",
    "print('mean of r_a_single: ', np.mean(r_a_single))\n",
    "print('mean of r_a_non_single: ', np.mean(r_a_non_single))\n",
    "print('mean of r_v_single: ', np.mean(r_v_single))\n",
    "print('mean of r_v_non_single: ', np.mean(r_v_non_single))\n",
    "fig.savefig('../../figures/unimodal_vs_multimodl_correlations_results_50_trials_mlv.pdf', bbox_inches='tight', transparent=True, format='pdf')\n",
    "\n",
    "\n",
    "# ANOVA Variance analysis\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "a = len(r_a_single)\n",
    "b = len(r_v_single)\n",
    "c = len(r_a_non_single)\n",
    "d = len(r_v_non_single)\n",
    "\n",
    "angles = np.array(['angle']*a) \n",
    "velocities = np.array(['velocity']*b)\n",
    "m = np.concatenate((angles, velocities))\n",
    "\n",
    "angles = np.array(['angle']*c) \n",
    "velocities = np.array(['velocity']*d)\n",
    "n = np.concatenate((angles, velocities))\n",
    "\n",
    "modals = np.array(['unimodal']*(a+b))\n",
    "non_modals = np.array(['multimodal']*(c+d))\n",
    "\n",
    "df = pd.DataFrame({'A': np.concatenate((m,n)),\n",
    "                    'B': np.concatenate((modals, non_modals)),\n",
    "                    'Y': np.concatenate((r_a_single, r_v_single, r_a_non_single, r_v_non_single))})\n",
    "\n",
    "formula = 'Y ~ C(A) + C(B) + C(A):C(B)'  \n",
    "model = ols(formula, df).fit()\n",
    "aov_table = anova_lm(model)\n",
    "print(aov_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
